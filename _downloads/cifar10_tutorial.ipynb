{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nTraining a Convolutional Neural Network for Image Classification\n================================================================\n\n*Tutorial adapted from http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html*\n\nGenerally, when you have to deal with image, text, audio or video data,\nyou can use standard python packages that load data into a numpy array.\nThen you can convert this array into a ``torch.*Tensor``.\n\n-  For images, packages such as Pillow, OpenCV are useful.\n-  For audio, packages such as scipy and librosa\n-  For text, either raw Python or Cython based loading, or NLTK and\n   SpaCy are useful.\n\nFor this tutorial, we will use the CIFAR10 dataset. It has the classes:\n``airplane``, ``automobile``, ``bird``, ``cat``, ``deer``, ``dog``,\n``frog``, ``horse``, ``ship``, ``truck``. The images in CIFAR-10 are of\nsize 3x32x32, i.e.\u00a03-channel color images of 32x32 pixels in size.\n\n.. figure:: /_static/cifar10.png\n   :alt: cifar\n\n   cifar\n\nTraining an image classifier\n----------------------------\n\nWe will do the following steps in order:\n\n1. Import libraries and add model settings\n2. Load and normalizing the CIFAR10 training and test datasets using\n   ``torchvision``\n3. Define a Convolution Neural Network (forward)\n4. Train the network on the training data\n5. Test the network on the test data\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Importing and Declaring Global Variables\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWe start be importing all libraries that will be required in this\ntutorial.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom cogitare.data import DataSet, AsyncDataLoader\nfrom cogitare import utils, Model\nfrom cogitare.plugins import EarlyStopping\nfrom cogitare.metrics.classification import accuracy\nimport cogitare\n\nimport argparse\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nparser = argparse.ArgumentParser()\npa = parser.add_argument  # define a shortcut\n\npa('--batch-size', help='Size of the training batch', type=int, default=64)\npa('--cuda', help='enable cuda', action='store_true')\npa('--dropout', help='dropout rate in the input data', type=float, default=0.3)\npa('--learning-rate', help='learning rate', type=float, default=0.001)\npa('--max-epochs', help='limit the number of epochs in training', type=int, default=10)\n\n\n# load the model arguments\ntry:\n    args = parser.parse_args()\nexcept:\n    args = parser.parse_args([])\n\n\ncogitare.utils.set_cuda(args.cuda)\n\nCLASSES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Loading and normalizing CIFAR10\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nUsing ``torchvision``, it\u2019s extremely easy to load CIFAR10.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The output of torchvision datasets are PILImage images of range [0, 1].\n# We transform them to Tensors of normalized range [-1, 1]\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n# load the CIFAR 10 data\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n                                        download=True, transform=transform)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, \n                                       download=True, transform=transform)\n\nprint(type(trainset.train_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Torchvision loads the data as numpy arrays.\n\nWe now create two datasets to hold the train and test sets.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(type(trainset.train_data))\n\nprint(type(trainset.train_labels))\n\ndef batch2variable(batch):\n    data, label = batch\n    data = utils.to_tensor(data)\n    \n    # B x W x H x C to B x C x W x W\n    data = data.transpose(1, 2).transpose(1, 3)\n    \n    return utils.to_variable(data, dtype=torch.FloatTensor), utils.to_variable(label)\n\n# convert the trainset.train_labels to LongTensor, instead of python list\n\ndata_train = DataSet([trainset.train_data, torch.LongTensor(trainset.train_labels)],\n                     batch_size=args.batch_size,\n                     drop_last=True)\n\n# use the async loader, to pre-load 8 batches ahead of the model\n# each batch is then loaded and moved to a torch Variable\ndata_train = AsyncDataLoader(data_train, buffer_size=args.batch_size * 8,\n                             on_batch_loaded=batch2variable)\n\ndata_test = DataSet([testset.test_data, torch.LongTensor(testset.test_labels)],\n                    batch_size=args.batch_size,\n                    drop_last=True)\ndata_test = AsyncDataLoader(data_test, buffer_size=args.batch_size * 8,\n                            on_batch_loaded=batch2variable)\n\n# fill the data buffer\ndata_train.cache()\ndata_test.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The train and test datasets are defined as a collection of tuples, each\ntuple contains ``(data, expected label)``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(next(data_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.1 Data Visualization\n----------------------\n\nLet us show some of the training images, for fun.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def imshow(img):\n    npimg = img.cpu().numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n\ndef showlabels(labels, qtd):\n    for j in range(1, qtd + 1):\n        print('%10s' % CLASSES[int(labels[j - 1])], end='')\n\n        if j % 4 == 0:\n            print('\\n')\n\nimages, labels = next(data_train)\nprint(images.shape)\n\nimshow(torchvision.utils.make_grid(images.data[:16], nrow=4))\nshowlabels(labels, 16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Define a Convolution Neural Network\n--------------------------------------\n\nIn this section, we\u2019ll define the forward method of the Cogitare Model.\nIn Cogitare, you must implement two methods in the model: **forward**\nand **loss**.\n\nThis is a Convolutional Neural Network (CNN) for Image Classification.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class CNN(Model):\n    \n    def __init__(self):\n        super(CNN, self).__init__()\n            \n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, batch):\n        # in this sample, each batch will be a tuple\n        # containing (input_batch, expected_batch)\n        # in forward in are only interested in input so that we \n        # can ignore the second item of the tuple\n        x, _ = batch      \n        \n        x = F.dropout(x, args.dropout)\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n\n        return F.log_softmax(x, dim=1)\n    \n    def loss(self, output, batch):\n        # in this sample, each batch will be a tuple \n        # containing (input_batch, expected_batch)\n        # in loss in are only interested in expected so that\n        # we can ignore the first item of the tuple\n        _, expected = batch\n\n        return F.nll_loss(output, expected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model class is simple; it only requires de forward and loss methods.\nBy default, Cogitare will backward the loss returned by the loss()\nmethod, and optimize the model parameters.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Training the Model\n---------------------\n\nWe first define the model optimizer for training the model.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cnn = CNN()\n\noptimizer = optim.Adam(cnn.parameters(), lr=args.learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now add the default plugins to watch the training status. The default\nplugin includes:\n\n-  Progress bar per batch and epoch\n-  Plot training and validation losses (if validation_dataset is\n   present)\n-  Log training loss\n\nAnd some extra plugins.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cnn.register_default_plugins()\n\nearly = EarlyStopping(max_tries=5, path='/tmp/model.pt')\n# after 5 epochs without decreasing the loss, stop the \n# training and the best model is saved at /tmp/model.pt\n\n# the plugin will execute in the end of each epoch\ncnn.register_plugin(early, 'on_end_epoch')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now run the training:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if args.cuda:\n    cnn = cnn.cuda()\n\ncnn.learn(data_train, optimizer, data_test, max_epochs=args.max_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Model Evaluation\n-------------------\n\nWe now check the model loss and accuracy on the test set:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def model_accuracy(output, data):\n    _, indices = torch.max(output, 1)\n\n    return accuracy(indices, data[1])\n\n# evaluate the model loss and accuracy over the validation dataset\nmetrics = cnn.evaluate_with_metrics(data_test, {'loss': cnn.metric_loss, 'accuracy': model_accuracy})\n\n# the metrics is an dict mapping the metric name (loss or accuracy, in this sample) to a list of the accuracy output\n# we have a measurement per batch. So, to have a value of the full dataset, we take the mean value:\n\nmetrics_mean = {'loss': 0, 'accuracy': 0}\nfor loss, acc in zip(metrics['loss'], metrics['accuracy']):\n    metrics_mean['loss'] += loss\n    metrics_mean['accuracy'] += acc.data[0]\n\nqtd = len(metrics['loss'])\n\nprint('Loss: {}'.format(metrics_mean['loss'] / qtd))\nprint('Accuracy: {}'.format(metrics_mean['accuracy'] / qtd))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.1 Visualization\n~~~~~~~~~~~~~~~~~\n\nTo check how the model behaves, we can plot the images, the expected and\npredicted labels\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "images, labels = next(data_test)\n\nimshow(torchvision.utils.make_grid(images.data[:16], nrow=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We forward the data to get the model output to the batch above.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predicted = cnn.predict((images, None))\n# remember that forward method expect a tuple, where the \n# first item contains the data to be passed in the net\npredicted.shape\n\n_, predicted_labels = torch.max(predicted, dim=1)\nprint('Predicted:\\n')\nshowlabels(predicted_labels[:16], 16)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}